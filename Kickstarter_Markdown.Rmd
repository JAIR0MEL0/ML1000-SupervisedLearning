---
title: "Kickstarter - Markdown"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

## Problem statement
Our customers are new entrepreneurs recently graduated from universities around the world who would like to have one of his ideas being sponsored by Kickstarter.  Before commiting meeting and efforts with Kickstarters they would like to know how successful their ideas are sponsored by Kickstarters Investors.


Gathering the data:
The data is downloaded from the Kaggle site where lots of dataset is available for analysis.  We identified Kickstarter as a great opportunity to learn and use Supervise Learning algorithms.
<https://www.kaggle.com/kemical/kickstarter-projects/home>
 

#Data Preparation
The Purpose of the data set is to identify what type of project are successfully sponsored.  The data collected from Kaggle has two different year of Kickstarters: 2016 and 2018.  We choose the most recent data as a way to create better predictions.

Analysis:
We starting by loading the data into a data variable.

```{r libraries, echo=FALSE}
library(lattice)
library(ggplot2)
library(caret)
library(plotly)
library(ggplot2)
library(randomForest)
library(dplyr)
getwd();
data <- read.csv("~/desktop/ML/YORK/Assigment1/kickstarter-projects/ks-projects-201801.csv", header = TRUE, dec = ".")
```

Let's do quick inspection of the data set:

```{r}
ncol(data);
nrow(data);
```
quickly preview data structure with HEAD(), STR(), and SUMMARY()
```{r}
head(data,10)
str(data)
summary(data)
```
#List of Features 
Here is the definition and relevance for each feature:

#ID
Identificator of the rows.  This feature does not contains any relevant value and will be ingnore for this analysis.
#Name
Description of the project or product.  This is not a nominal feature we can use; we will ignore it for our analysis.
#Category
This is a classification associated to the type of project.  This is a relevant feature for analysis.
#Main Category
Group of Categories.  Also a relevant feature for our analysis.
#Currency
Currency of the country where the project will be developed.  Currentcy and Country are giving the same information; so we will omit this feature during our analysis.
#Dead Line
This is the date when the goal money should be collected for the project to start.  We will consider this feature to create a new feature.
#Goal
Minimum Amount of money targeted to be used for the development of the project.  We will use this feature for our initial analysis.  However, instead of using the Goal we will use the USD Goal Real as it has already converted into common currency, USD.
#Launched
Date when the project is presented to kickstarters for funding. We identify this as a potential preductor in conjuction with the Deadline.
We will create the new feature called Campaign, which is a differen between the Day the Campaign to collect the goal started and the deadline when the funding #campaign:
The numbers of days between the launch and the deadline which is when the campaign is over.
```{r}
data[["campaign"]] <- as.integer(difftime(data$deadline ,data$launched , units = c("days")))
```
#Pledged
Money amount finally granted for the project in the currency of the country.  Pledged has a strong correlation with Goal for those succesful project; failed will mean, a project has not achieve the goal.  In this case, we will omit Pledged and use the goal.
#State
Status of the project is the label of the data set, contains the values to determine whether the project is successful or not.  
#Backers
Number of sponsors.  This a relevant information for our initial analysis as we can determine whether there is a relationship between the number of backers for successful projects.
#Country
Place where the project is being developed.  We will use this feature during our initial analysis as we would like to determine if there is a relationship with the country for all the succesful projects.
#USD Pledged
This the amount granted in USD.  We will ignore this feature as there is strong correlation with the USD Goal Real.
#USD Pledged Real
This the amount granted in USD.  We will ignore this feature as there is strong correlation with the USD Goal Real.
#USD Goal Real
This the goal amount in USD. We will use this feature for our initial analysis.

##Cleaning Data
We will clean any records with missing data in order to increase accuracy of the model

Before Deletion
```{r}
nrow(data);
sum(!complete.cases(data))
data = data[complete.cases(data),]
```
After Deletion
```{r}
nrow(data);
sum(!complete.cases(data))
```

##Understanding the Data
Let's see what are the type of states are recorded in the data set to identify potential unrelevant.

We will start inspecting the state column distribuition that might will be our key to understand this dataset
```{r echo=FALSE}
pieState <- table(data$state)
pct <- round(pieState/sum(pieState)*100)
lbls <- paste(names(pieState), "\n", pieState, sep="")
lbls <- paste(lbls, pct) # add percents to labels 
lbls <- paste(lbls,"%",sep="") # ad % to labels 
pie(pieState,labels = lbls, col=rainbow(length(lbls)),
    main="Pie Chart by State")
```
As we can see, there are only two relevant of our outcome variable: Successful and Failed
```{r}
percentage <- prop.table(table(data$state)) * 100
cbind(freq=table(data$state), percentage)

```
Then... Let's clear records with state different than Succesfull or Failed.

```{r}
data <- data[ which(data$state=='successful' 
                         | data$state =='failed'), ]
data$state <- factor(data$state)
data$country <- factor(data$country)
```
# Now let;s look at the distributions of each predictor

# Backers
```{r echo=FALSE}
ggplot(data=data[c("backers")],aes(x=backers))+geom_histogram(aes(y=..count../sum(..count..)),breaks=seq(0,100,5),col="black",fill='blue') +
 ggtitle('Normalized distribution of number of backers for all the projects')
```

# usd goal real

```{r echo=FALSE}
ggplot(data=data[c("usd_goal_real")],aes(x=usd_goal_real))+geom_histogram(aes(y=..count../sum(..count..)),breaks=seq(0,20000,1000),col="black",fill='blue') +
ggtitle('Normalized distribution of the goal (USD) for all the projects')
```
As we see a project has more probability to failed with a high goal

# Campaign
```{r echo=FALSE}
ggplot(data=data[c("campaign")],aes(x=campaign))+geom_histogram(aes(y=..count../sum(..count..)),breaks=seq(0,100,5),col="black",fill='blue')+
ggtitle('Normalized distribution of number of campaign days for all the projects')
```

# category
```{r, echo=FALSE}
ggplot(data.frame(data), aes(x=main_category)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='blue') + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
    ggtitle('Normalized distribution of number of projects for each category')
```

# country
```{r echo=FALSE}
ggplot(data.frame(data), aes(x=country)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='blue') + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
ggtitle('Normalized distribution of number of projects for each country')
```

# But this does not  give an indication of predictive power of each feature. Look at the 
# Fraction of failed and successful projects in each bin of each predictor.

```{r echo=FALSE}
ggplot() + geom_histogram(data=data, aes(x=backers,fill=state),position = "fill",breaks=seq(0,100,5))
```
```{r echo=FALSE}
ggplot() + geom_histogram(data=data, aes(x=usd_goal_real,fill=state),position = "fill",breaks=seq(0,20000,1000))
```

```{r echo=FALSE}
ggplot() + geom_histogram(data=data, aes(x=campaign,fill=state),position = "fill",breaks=seq(0,92,5))
```


```{r echo=FALSE}
ggplot(data.frame(data), aes(main_category,fill=state)) + geom_bar( position="fill") + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
```


Now, Let's analyze the relationship between backers and the successful projects per categories.  As we think, there is a strong relationship between the number of backers and the success of projects.  We should consider Backers as a strong predictor for succesfull projects.
```{r echo=FALSE}
ggplot(data.frame(data), aes(country,fill=state)) + geom_bar( position="fill") + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
```

## What features made the final list
Main Category groups, Backers, Country, Goal in USD, Campaign are the predictive variables, while State is the target variable.

Eliminate features that are duplicates
```{r}
data <- select(data,-name)
data <- select(data,-ID)
data <- select(data,-category)
data <- select(data,-deadline)
data <- select(data,-launched)
data <- select(data,-usd.pledged)
data <- select(data,-usd_pledged_real)
data <- select(data,-pledged)
data <- select(data,-goal)
```

# Feature selection
```{r}
rfImp <- randomForest(state ~ ., data = data, ntree = 100, importance = TRUE)
importance(rfImp,type=2)
```

## Preparatoion of Training and Test data

We will split the data between training and testing.

```{r}

# split data into training and testing chunks
set.seed(1234)

# generalize outcome and predictor variables
outcomeName <- 'state'

#This is to use the same data set for Training and Test
splitIndex <- createDataPartition(data[,outcomeName], p = .75, list = FALSE, times = 1)
trainDF <- data[ splitIndex,]
testDF  <- data[-splitIndex,]
summary(trainDF)
summary(testDF)
#Dimensions of the dataset
dim(trainDF)
```

What are the types of attribues:
```{r}
sapply(trainDF, class)
```

##Evaluating the Models

Let's review the models.  General Linear Model Start with Lineal regression model

GLM is a supervised algorithm with a classic statistical technique (Supports thousands of input variables, text and transactional data) used for:  Classification and/or Regression

# a) LDA: Linear Discriminant Analysis
```{r}
trctl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE)
metric <- "Accuracy"

#Removing Country as there is Country has low predictibility and it's values are in collinearity with State.
set.seed(152)
fit.lda <- train(state ~ main_category + usd_goal_real + backers + campaign + country, data=trainDF, method="lda", metric=metric, trControl=trctl)

```
# b) Nonlinear algorithms Classification Tree / Recursive Partitioning
```{r}
set.seed(152)
fit.cart <- train(state ~ main_category + usd_goal_real + backers + campaign + country, data=trainDF, method="rpart", metric=metric, trControl=trctl)
```
# c) Final algorithm Random Forest
```{r}
set.seed(152)
fit.rf <- train(state ~ main_category + usd_goal_real + backers + campaign + country, data=trainDF, method="rf", metric=metric, trControl=trctl)
```

## Evalutate model
#################################################
summarize accuracy of models (Training)
```{r}
results <- resamples(list(lda=fit.lda, cart=fit.cart, rf=fit.rf))
summary(results)
```
## Visualize the accuracy of the models
```{r}
dotplot(results)
```
##Let's make a prediction (accuracy of testing dataset)

#a) Linear Discriminant Analysis and Confusion Matrix
```{r}
predictions <- predict(fit.lda, testDF)
head(predictions)
confusionMatrix(predictions,testDF$state)
```
#b) Classification Tree / Recursive Partitioning and Confusion Matrix
```{r}
predictions <- predict(fit.cart, testDF)
head(predictions)
confusionMatrix(predictions,testDF$state)
```
#c) Final algorithm Random Forest and confusion matrix
```{r}
predictions <- predict(fit.rf, testDF)
head(predictions)
confusionMatrix(predictions,testDF$state)
```

## Deployment
Predicting outcome of an unseen data point
```{r}
xnew = data[211701,c('main_category','backers','country','usd_goal_real','campaign')]
predict(fit.rf,xnew)
```
#Evaluating Models
Now it's time to evaluate the models:
```{r}
results <- resamples(list(lda=fit.lda, cart=fit.cart, rf=fit.rf))
summary(results)

results <- resamples(list(lda=fit.lda, cart=fit.cart, rf=fit.rf))
summary(results)
```

## Visualize the accuracy of the models

```{r echo=FALSE}
dotplot(results)
```
#summarize best model
```{r echo=FALSE}
print(fit.rf)
```
##Running a Testing
Estimate skill of Random Forest on trainDF dataset
```{r}
predictions <- predict(fit.rf, testDF)
head(predictions)
confusionMatrix(predictions,testDF$state)
```

##Conclusion:
Using the five predictors: main_category, backers, country, usd_goal_real, campaign and the label state we were able to create a model that predicts with 9.1% of accuracy whether a project will be successful or not.
Main Category and the Goal amount have a higher impact in the success of a Project.



