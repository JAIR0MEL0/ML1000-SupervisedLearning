---
title: "Kickstarter - Markdown"
output: html_document
---

## Problem statement
Our customers are new entrepreneurs recently graduated from universities around the world who would like to have one of his ideas being sponsored by Kickstarter.  Before commiting meeting and efforts with Kickstarters they would like to know how successful their ideas are sponsored by Kickstarters Investors.


Gathering the data:
The data is downloaded from the Kaggle site where lots of dataset is available for analysis.  We identified Kickstarter as a great opportunity to learn and use Supervise Learning algorithms.
<https://www.kaggle.com/kemical/kickstarter-projects/home>
 

#Data Preparation
The Purpose of the data set is to identify what type of project are successfully sponsored.  The data collected from Kaggle has two different year of Kickstarters: 2016 and 2018.  We choose the most recent data as a way to create better predictions.

Analysis:
We starting by loading the data into a data variable.

```{r}
library(lattice)
library(ggplot2)
library(caret)
library(plotly)
#install.packages("randomForest")
#library(randomForest)
getwd();
data <- read.csv("~/desktop/ML/YORK/Assigment1/kickstarter-projects/ks-projects-201801.csv", header = TRUE, dec = ".")
```

Let's do quick inspection of the data set:

```{r}
ncol(data);
nrow(data);
```
quickly preview data structure with HEAD(), STR(), and SUMMARY()
```{r State}
head(data,10)
str(data)
summary(data)
```
#List of Features 
Here is the definition and relevance for each feature:

ID: Identificator of the rows.  This feature does not contains any relevant value and will be ingnore for this analysis.
Name: Description of the project or product.  This is not a nominal feature we can use; we will ignore it for our analysis.
Category: This is a classification associated to the type of project.  This is a relevant feature for analysis.
Main Category: Group of Categories.  Also a relevant feature for our analysis.
Currency: Currency of the country where the project will be developed.  There is a strong correlation between Currentcy and Country; so we will omit this feature during our analysis.
Dead Line: This is the date when the goal money should be collected for the project to start.  We will consider this feature to create a new feature.
Goal: Minimum Amount of money targeted to be used for the development of the project.  We will use this feature for our initial analysis.  However, instead of using the Goal we will use the USD Goal Real as it has already converted into common currency, USD.
Launched: Date when the project is presented to kickstarters for funding. We identify this as a potential preductor in conjuction with the Deadline.
We will create the new feature called Campaign, which is a differen between the Day the Campaign to collect the goal started and the deadline when the funding campaign is over:
```{r}
data[["campaign"]] <- as.integer(difftime(data$deadline ,data$launched , units = c("days")))
```
Pledged: Money amount finally granted for the project in the currency of the country.  Pledged has a strong correlation with Goal for those succesful project; failed will mean, a project has not achieve the goal.  In this case, we will omit Pledged and use the goal.
State: Status of the project is the label of the data set, contains the values to determine whether the project is successful or not.  
Backers: Number of sponsors.  This a relevant information for our initial analysis as we can determine whether there is a relationship between the number of backers for successful projects.
Country: Place where the project is being developed.  We will use this feature during our initial analysis as we would like to determine if there is a relationship with the country for all the succesful projects.
USD Pledged: This the amount granted in USD.  We will ignore this feature as there is strong correlation with the USD Goal Real.
USD Pledged Real:  This the amount granted in USD.  We will ignore this feature as there is strong correlation with the USD Goal Real.
USD Goal Real: This the goal amount in USD. We will use this feature for our initial analysis.

## Cleaning Data
#We will clean any records with missing data.
#Before Deletion
```{r}
nrow(data);
sum(!complete.cases(data))
data = data[complete.cases(data),]
#After Deletion
sum(!complete.cases(data))
```
# Total number of levels in target variable state.
```{r}
levels(data$state)
```
Then... Let's delete records with state different than Succesfull or Failed.

```{r}
data <- data[ which(data$state=='successful' | data$state =='failed'), ]
data$state <- factor(data$state, labels = c('successful', 'failed'))
levels(data$state)
```

# What features made the final list
Main Category groups, Backers, Country, Goal in USD, Campaign and state as predictive variables
```{r}
#eliminate features that are duplicates
data <- data[c("main_category","backers","country","usd_goal_real","campaign","state")]
```

## Data exploration
#To begin with look at the distributions of target variable (binary)

```{r state, echo=FALSE}
pieState <- table(data$state)
pct <- round(pieState/sum(pieState)*100)
lbls <- paste(names(pieState), "\n", pieState, sep="")
lbls <- paste(lbls, pct) # add percents to labels 
lbls <- paste(lbls,"%",sep="") # ad % to labels 
pie(pieState,labels = lbls, col=rainbow(length(lbls)),main="Pie Chart by State")
```

# Look at the distributions of each predictor
# Backers
```{r state, echo=FALSE}
ggplot(data=data[c("backers")],aes(x=backers))+geom_histogram(aes(y=..count../sum(..count..)),breaks=seq(0,100,5),col="black",fill='blue') +
 ggtitle('Normalized distribution of number of backers for all the projects')
```

# goal
```{r state, echo=FALSE}
ggplot(data=data[c("usd_goal_real")],aes(x=usd_goal_real))+geom_histogram(aes(y=..count../sum(..count..)),breaks=seq(0,20000,1000),col="black",fill='blue') +
ggtitle('Normalized distribution of the goal (USD) for all the projects')
```

# Campaign
```{r state, echo=FALSE}
ggplot(data=data[c("campaign")],aes(x=campaign))+geom_histogram(aes(y=..count../sum(..count..)),breaks=seq(0,100,5),col="black",fill='blue')+
ggtitle('Normalized distribution of number of campaign days for all the projects')
```

# category
```{r state, echo=FALSE}
ggplot(data.frame(data), aes(x=main_category)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='blue') + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
    ggtitle('Normalized distribution of number of projects for each category')
```

# country
```{r state, echo=FALSE}
ggplot(data.frame(data), aes(x=country)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='blue') + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
ggtitle('Normalized distribution of number of projects for each country')
```

# Currency
```{r state, echo=FALSE}
ggplot(data.frame(data), aes(x=currency)) + geom_bar(aes(y=..count../sum(..count..)),col='black',fill='blue') + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5)) +
ggtitle('Normalized distribution of number of projects for each currency')
```

# But this does not  give an indication of predictive power of each feature. Look at the 
# Fraction of failed and successful projects in each bin of each predictor.
```{r state, echo=FALSE}
ggplot(data.frame(data), aes(main_category,fill=state)) + geom_bar( position="fill") + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
```

```{r state, echo=FALSE}
ggplot(data.frame(data), aes(country,fill=state)) + geom_bar( position="fill") + theme(axis.text.x=element_text(angle=90,hjust=1,vjust=0.5))
```

```{r state, echo=FALSE}
ggplot() + geom_histogram(data=data, aes(x=backers,fill=state),position = "fill",breaks=seq(0,100,5))
```

```{r state, echo=FALSE}
ggplot() + geom_histogram(data=data, aes(x=usd_goal_real,fill=state),position = "fill",breaks=seq(0,20000,1000))
```

```{r state, echo=FALSE}
ggplot() + geom_histogram(data=data, aes(x=campaign,fill=state),position = "fill",breaks=seq(0,100,5))
```

# Feature selection
```{r}
rfImp <- randomForest(state ~ ., data = data, ntree = 200, importance = TRUE)
importance(rfImp,type=2)
```
#Backers have highest importance. Since all features have nonzero importance retain all.

## Preparatoion of Training and Test data
```{r}
# split data into training and testing chunks
set.seed(1234)

# generalize outcome and predictor variables
outcomeName <- 'state'

#This is to use the same data set for Training and Test
splitIndex <- createDataPartition(data[,outcomeName], p = .75, list = FALSE, times = 1)
trainDF <- data[ splitIndex,]
testDF  <- data[-splitIndex,]
summary(trainDF)
summary(testDF)
#Dimensions of the dataset
dim(trainDF)
```

What are the types of attribues:
```{r}
sapply(trainDF, class)
```


How would you measure the effectiveness of a good prediction algorithm or clustering algorithm?
Describe the final dataset that is used for classification (include a description of any newly formed variables you created).

```{r}




```
##Evaluating the Models

Let's review the models.  General Linear Model Start with Lineal regression model

GLM is a supervised algorithm with a classic statistical technique (Supports thousands of input variables, text and transactional data) used for:  Classification and/or Regression

# a) Linear Discriminant Analysis

```{r}
trctl <- trainControl(method = 'cv', number = 10, savePredictions = TRUE)
metric <- "Accuracy"

set.seed(7)
fit.lda <- train(state ~ main_category + goal + backers + campaign, data=trainDF, method="lda", metric=metric, trControl=trctl)
```
Nonlinear algorithms
Classification Tree / Recursive Partitioning```
```{r}
set.seed(7)
fit.cart <- train(state ~ main_category + goal + backers + campaign, data=trainDF, method="rpart", metric=metric, trControl=trctl)
```

Random Forest
```{r}
set.seed(7)
fit.rf <- train(state ~ main_category + goal + backers + campaign, data=trainDF, method="rf", metric=metric, trControl=trctl)
```
#Evaluating Models
Now it's time to evaluate the models (training):
#################################################
```{r}
results <- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)

#Visualize the accuracy of the models
dotplot(results)
#summarize best model
print(fit.rf)
```

##Running a Testing
Estimate skill of Random Forest on testDF dataset
```{r}
predictions <- predict(fit.rf, testDF)
head(predictions)
confusionMatrix(predictions,testDF$successful)


```

Conclusion:


